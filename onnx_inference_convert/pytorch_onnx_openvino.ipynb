{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwEAhQVzkAwA"
   },
   "source": [
    "# OpenVINO ONNX demo\n",
    "\n",
    "This tutorial demostrates step-by-step instructions to perform inference on a PyTorch model using [OpenVINO](https://github.com/openvinotoolkit/openvino)\n",
    "\n",
    "The PyTorch model is converted to ONNX and loaded with OpenVINO. The model is pretrained on [CityScapes](https://www.cityscapes-dataset.com). The model source is https://github.com/ekzhang/fastseg\n",
    "\n",
    "NOTE: This is a work-in-progress tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filenames of the downloaded and converted models\n",
    "base_model_name = \"fastseg\"\n",
    "model_fname = base_model_name + \".pth\"\n",
    "onnx_fname = base_model_name + \".onnx\"\n",
    "ir_fname = base_model_name + \".xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6DALKzhsj-p"
   },
   "source": [
    "### Install OpenVINO and the required Python packages\n",
    "\n",
    "Download and Install the required packages for this demo. If you use OSX, change this cell to use the command to install PyTorch for OSX.\n",
    "\n",
    "There may be some errors in the output because of version conflicts. The notebook should work fine despite these version conflicts.\n",
    "\n",
    "After running this cell and installing the packages, you can delete this cell or comment out the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgGsysZFiAAm",
    "outputId": "5ed2d194-3886-44eb-fa9b-fc177df42d3c"
   },
   "outputs": [],
   "source": [
    "# OpenVINO\n",
    "!pip install --upgrade --quiet --index-url https://test.pypi.org/simple --extra-index-url https://pypi.org/simple openvino-dev\n",
    "\n",
    "# PyTorch\n",
    "## Windows and Linux\n",
    "!pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "## OSX\n",
    "# pip install torch==1.5.1 torchvision==0.6.1\n",
    "\n",
    "# Model specific dependencies\n",
    "!pip install -U --quiet numpy==1.18.5 geffnet==0.9.8 fastseg onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN9dKeKiz4Ut"
   },
   "source": [
    "## Image Segmentation with Fastseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB4Yo-rGGLmV"
   },
   "source": [
    "### Import the PyTorch Library and Fastseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ynWRum4iiTz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from fastseg import MobileV3Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5xKw0hR0jq6"
   },
   "source": [
    "### Download the Fastseg Model\n",
    "\n",
    "This downloads and loads the model and pretrained weights. It may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGKkMRfvi0op",
    "outputId": "4eb1f9af-a4c5-424c-f808-dd9cc2600975"
   },
   "outputs": [],
   "source": [
    "print(\"Downloading the Fastseg model (if it has not been downloaded before)....\")\n",
    "model = MobileV3Large.from_pretrained().cpu().eval()\n",
    "model.eval()\n",
    "print(\"Loaded PyTorch Fastseg model\")\n",
    "\n",
    "# Save the model\n",
    "if not os.path.exists(model_fname):\n",
    "    print(\"\\nSaving the model\")\n",
    "    torch.save(model.state_dict(), model_fname)\n",
    "    print(f\"Model saved at {model_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rhc_7EObUypw"
   },
   "source": [
    "### Create ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipQWpbgQUxoo",
    "outputId": "bbc1734a-c2a2-4261-ed45-264b9e3edd00"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(onnx_fname):\n",
    "    dummy_input = torch.randn(1, 3, 512, 1024)\n",
    "    torch.onnx.export(model, dummy_input, onnx_fname, opset_version=11)\n",
    "    print(f\"ONNX model exported to {onnx_fname}.\")\n",
    "else:\n",
    "    print(f\"ONNX model {onnx_fname} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JSoEIk60uxV"
   },
   "source": [
    "### Convert the Model to IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the Model Optimizer script\n",
    "import mo_onnx\n",
    "\n",
    "mo_path = str(Path(mo_onnx.__file__).with_name(\"mo.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YUwrq7QWSzw"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(ir_fname):\n",
    "    print(\"Exporting ONNX model to IR...\")\n",
    "    ! python $mo_path --input_model $onnx_fname --input_shape \"[1,3, 512,1024]\" --data_type FP16 --output_dir . --model_name $base_model_name\n",
    "    print(f\"ONNX model exported to IR model: {ir_fname}\")\n",
    "else:\n",
    "    print(f\"IR model {ir_fname} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAGmlKQ83ecE"
   },
   "source": [
    "### Define Preprocessing and Display Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTOoQnSetzQM"
   },
   "outputs": [],
   "source": [
    "def normalize(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize the image to the given mean and standard deviation for CityScapes models.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    image /= 255.0\n",
    "    image -= mean\n",
    "    image /= std\n",
    "    return image\n",
    "\n",
    "\n",
    "def show_image_and_result(image: np.ndarray, result: np.ndarray):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(25, 8))\n",
    "    ax[0].imshow(image)\n",
    "    ax[1].imshow(result)\n",
    "    for a in ax:\n",
    "        a.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyD5EKka34Wd"
   },
   "source": [
    "### Load and Pre-process an Input Image\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGFW5VXL3x9G",
    "outputId": "300eacff-c6de-4eb5-e99a-8def5260da1a"
   },
   "outputs": [],
   "source": [
    "# image source: https://www.flickr.com/people/roland/ via https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&type=segmentation&r=false&c=%2Fm%2F0k4j&id=531b67238c25813b CC BY 2.0\n",
    "image_url = \"https://c4.staticflickr.com/4/3901/14855908765_75af9fac24_o.jpg\"\n",
    "urllib.request.urlretrieve(image_url, \"image.jpg\")\n",
    "image = cv2.cvtColor(cv2.imread(\"image.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "resized_image = cv2.resize(image, (1024, 512))\n",
    "normalized_image = normalize(resized_image)\n",
    "input_image = np.expand_dims(np.transpose(normalized_image, (2, 0, 1)), 0).astype(\n",
    "    np.float32\n",
    ")  # Convert the image shape to shape expected by network\n",
    "input_image_tensor = torch.as_tensor(input_image).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnEiEbNq4Csh"
   },
   "source": [
    "### Load the OpenVINO IR network and Run Inference on the ONNX model\n",
    "\n",
    "Inference Engine can load ONNX models directly. We first load the ONNX model, do inference and show the results. After that we load the model that was converted to Intermediate Representation (IR) with Model Optimizer and do inference on that model and show the results.\n",
    "\n",
    "#### 1. ONNX model in Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otfT6EDk03KV"
   },
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "\n",
    "# Load network to the plugin\n",
    "ie = IECore()\n",
    "net_onnx = ie.read_network(model=\"fastseg.onnx\")\n",
    "exec_net_onnx = ie.load_network(network=net_onnx, device_name=\"CPU\")\n",
    "\n",
    "input_layer_onnx = next(iter(exec_net_onnx.input_info))\n",
    "output_layer_onnx = next(iter(exec_net_onnx.outputs))\n",
    "\n",
    "# Run the Inference on the Input image...\n",
    "res_onnx = exec_net_onnx.infer(inputs={input_layer_onnx: input_image})\n",
    "res_onnx = res_onnx[output_layer_onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "q8NRH8hLuWeV",
    "outputId": "8b17f90e-b3bc-456f-b6f3-e9cd4f743b2f"
   },
   "outputs": [],
   "source": [
    "result_mask_onnx = np.squeeze(np.argmax(res_onnx, axis=1))\n",
    "show_image_and_result(image, result_mask_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnEiEbNq4Csh"
   },
   "source": [
    "#### 2. IR model in Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otfT6EDk03KV"
   },
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "\n",
    "# Load network to the plugin\n",
    "ie = IECore()\n",
    "net_ir = ie.read_network(model=ir_fname)\n",
    "exec_net_ir = ie.load_network(network=net_ir, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ir = next(iter(exec_net_ir.input_info))\n",
    "output_layer_ir = next(iter(exec_net_ir.outputs))\n",
    "\n",
    "# Run the Inference on the Input image...\n",
    "res_ir = exec_net_ir.infer(inputs={input_layer_ir: input_image})\n",
    "res_ir = res_ir[output_layer_ir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "q8NRH8hLuWeV",
    "outputId": "8b17f90e-b3bc-456f-b6f3-e9cd4f743b2f"
   },
   "outputs": [],
   "source": [
    "result_mask_ir = np.squeeze(np.argmax(res_ir, axis=1))\n",
    "show_image_and_result(image, result_mask_ir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3UUduQEGsQm"
   },
   "source": [
    "## PyTorch Comparison\n",
    "\n",
    "Do inference on the PyTorch model to verify that the output visually looks the same as the ONNX/IR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "1l1JtgeV4Wuw",
    "outputId": "f21c8904-83da-438c-df39-4620bb679554"
   },
   "outputs": [],
   "source": [
    "result_torch = model(input_image_tensor)\n",
    "result_mask_torch = torch.argmax(result_torch, dim=1).squeeze(0)\n",
    "show_image_and_result(image, result_mask_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison\n",
    "\n",
    "Measure the time it takes to do inference on five images. This gives an indication of performance. For more accurate benchmarking, use the [OpenVINO benchmark tool](https://github.com/openvinotoolkit/openvino/tree/master/inference-engine/tools/benchmark_tool). Note that many optimizations are possible to improve the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 5\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    exec_net_onnx.infer(inputs={input_layer_onnx: input_image})\n",
    "end = time.perf_counter()\n",
    "time_onnx = end - start\n",
    "print(f\"ONNX model in Inference Engine/CPU: {time_onnx/num_images:.3f} seconds per image, FPS: {num_images/time_onnx:.2f}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    exec_net_ir.infer(inputs={input_layer_ir: input_image})\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(f\"IR model in Inference Engine/CPU: {time_ir/num_images:.3f} seconds per image, FPS: {num_images/time_ir:.2f}\")\n",
    "\n",
    "# Uncomment these lines for GPU performance stats\n",
    "#\n",
    "# exec_net_onnx_gpu = ie.load_network(network=net_ir, device_name=\"GPU\")\n",
    "# start = time.perf_counter()\n",
    "# for _ in range(num_images):\n",
    "#     exec_net_onnx_gpu.infer(inputs={input_layer_onnx: input_image})\n",
    "# end = time.perf_counter()\n",
    "# time_onnx_gpu = end - start\n",
    "# print(f'ONNX model in Inference Engine/GPU: {time_onnx_gpu/num_images:.3f} seconds per image, FPS: {num_images/time_onnx_gpu:.2f}')\n",
    "\n",
    "# exec_net_ir_gpu = ie.load_network(network=net_ir, device_name=\"GPU\")\n",
    "# start = time.perf_counter()\n",
    "# for _ in range(num_images):\n",
    "#     exec_net_ir_gpu.infer(inputs={input_layer_ir: input_image})\n",
    "# end = time.perf_counter()\n",
    "# time_ir_gpu = end - start\n",
    "# print(f'IR model in Inference Engine/GPU: {time_ir_gpu/num_images:.3f} seconds per image, FPS: {num_images/time_ir_gpu:.2f}')\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    model(input_image_tensor)\n",
    "end = time.perf_counter()\n",
    "time_torch = end - start\n",
    "print(f\"PyTorch model on CPU: {time_torch/num_images:.3f} seconds per image, FPS: {num_images/time_torch:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show CPU Information for reference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cpuinfo\n",
    "\n",
    "    print(cpuinfo.get_cpu_info()[\"brand_raw\"])\n",
    "except:  # OpenVINO installs cpuinfo, but if a different version is installed the command above may not work\n",
    "    import platform\n",
    "\n",
    "    print(platform.processor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* PIP install openvino-dev: https://github.com/openvinotoolkit/openvino/blob/releases/2021/3/docs/install_guides/pypi-openvino-dev.md\n",
    "* OpenVINO ONNX support: https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_ONNX_Support.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OpenVINO 2021.3 PIP installer - PyTorch Image Segmentation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
