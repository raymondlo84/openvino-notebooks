{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwEAhQVzkAwA"
   },
   "source": [
    "# Introduction\n",
    "This tutorial demostrates step-by-step instructions to perform inference on a PyTorch model using [OpenVINO](https://github.com/openvinotoolkit/openvino)\n",
    "\n",
    "The PyTorch model is converted to ONNX and loaded with OpenVINO. The model is pretrained on [CityScapes](https://www.cityscapes-dataset.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "md6wxNypI4l2",
    "outputId": "69cd5256-4097-4d52-bd9f-c0da09b0b1c0"
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<div style=\"background-color:yellow; color:black;padding:10px;\">You need to upload the openvino and openvino_dev wheels before running this notebook. Upload files in the file browser section of the left sidebar. Also, you need to <b>set the path to the Model Optimizer</b> in the Settings.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Model Optimizer ONNX file. The full path to `mo_onnx.py` \n",
    "mo_path = r'C:\\Users\\havanden\\openvino_env\\Lib\\site-packages\\mo_onnx.py'\n",
    "\n",
    "# The filenames of the downloaded and converted models\n",
    "base_model_name = 'fastseg'\n",
    "model_fname = base_model_name + '.pth'\n",
    "onnx_fname = base_model_name + '.onnx'\n",
    "ir_fname = base_model_name + '.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install OpenVINO\n",
    "\n",
    "This is a temporary step that will no longer be necessary with OpenVINO 2021.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "openvino_pip = glob.glob('openvino-2021*whl')[0]\n",
    "openvino_pip_dev = glob.glob('openvino_dev*whl')[0]\n",
    "!pip install $openvino_pip $openvino_pip_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6DALKzhsj-p"
   },
   "source": [
    "# Install required Python packages\n",
    "\n",
    "Download and Install the required packages for this demo. If you use OSX, change this cell to use the command to install PyTorch for OSX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgGsysZFiAAm",
    "outputId": "5ed2d194-3886-44eb-fa9b-fc177df42d3c"
   },
   "outputs": [],
   "source": [
    "# Windows and Linux\n",
    "!pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# OSX\n",
    "# pip install torch==1.5.1 torchvision==0.6.1\n",
    "\n",
    "!pip install -U --quiet geffnet==0.9.8 fastseg onnx opencv-python-headless numpy==1.18.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN9dKeKiz4Ut"
   },
   "source": [
    "# OpenVINO PyTorch Demo - Image Segmentation with Fastseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB4Yo-rGGLmV"
   },
   "source": [
    "### Import the PyTorch Library and Fastseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ynWRum4iiTz"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from fastseg import MobileV3Large\n",
    "\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5xKw0hR0jq6"
   },
   "source": [
    "### Download the Fastseg Model\n",
    "\n",
    "This downloads and loads the model and pretrained weights. It may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGKkMRfvi0op",
    "outputId": "4eb1f9af-a4c5-424c-f808-dd9cc2600975"
   },
   "outputs": [],
   "source": [
    "print(\"Downloading the Fastseg model (if it has not been downloaded before)....\")\n",
    "model = MobileV3Large.from_pretrained().cpu().eval()\n",
    "model.eval()\n",
    "print(\"Loaded PyTorch Fastseg model\")\n",
    "\n",
    "# Save the model\n",
    "if not os.path.exists(model_fname):\n",
    "    print(\"\\nSaving the model\")\n",
    "    torch.save(model.state_dict(), model_fname)\n",
    "    print(f\"Model saved at {model_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rhc_7EObUypw"
   },
   "source": [
    "### Create ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipQWpbgQUxoo",
    "outputId": "bbc1734a-c2a2-4261-ed45-264b9e3edd00"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(onnx_fname):\n",
    "    dummy_input = torch.randn(1, 3, 512, 1024)\n",
    "    torch.onnx.export(model, dummy_input, onnx_fname, opset_version=11)\n",
    "    print(f'ONNX model exported to {onnx_fname}.')\n",
    "else:\n",
    "    print(f'ONNX model {onnx_fname} already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JSoEIk60uxV"
   },
   "source": [
    "### Convert the Model to IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YUwrq7QWSzw"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(ir_fname):\n",
    "    print('Exporting ONNX model to IR...')\n",
    "    ! python $mo_path --input_model $onnx_fname --input_shape \"[1,3, 512,1024]\" --data_type FP16 --output_dir . --model_name $base_model_name\n",
    "    print(f'ONNX model exported to IR model: {ir_fname}')\n",
    "else:\n",
    "    print(f'IR model {ir_fname} already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAGmlKQ83ecE"
   },
   "source": [
    "### Define Preprocessing and Display Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTOoQnSetzQM"
   },
   "outputs": [],
   "source": [
    "def normalize(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize the image to the given mean and standard deviation for CityScapes models.\"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    image /= 255.0\n",
    "    image -= mean\n",
    "    image /= std\n",
    "    return image\n",
    "\n",
    "def show_image_and_result(image:np.ndarray, result: np.ndarray):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(25,8))\n",
    "    ax[0].imshow(image)\n",
    "    ax[1].imshow(result)\n",
    "    for a in ax:\n",
    "        a.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyD5EKka34Wd"
   },
   "source": [
    "### Load and Pre-process an Input Image\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGFW5VXL3x9G",
    "outputId": "300eacff-c6de-4eb5-e99a-8def5260da1a"
   },
   "outputs": [],
   "source": [
    "image = cv2.cvtColor(cv2.imread(\"street.png\"), cv2.COLOR_BGR2RGB)\n",
    "resized_image = cv2.resize(image, (1024,512))\n",
    "normalized_image = normalize(resized_image)\n",
    "input_image = np.expand_dims(np.transpose(normalized_image, (2,0,1)), 0).astype(np.float32)  # Convert the image shape to shape expected by network\n",
    "input_image_tensor = torch.as_tensor(input_image).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnEiEbNq4Csh"
   },
   "source": [
    "### Load the OpenVINO IR network and Run Inference on the ONNX model\n",
    "\n",
    "Inference Engine can load ONNX models directly. We first load the ONNX model, do inference and show the results. After that we load the model that was converted to Intermediate Representation (IR) with Model Optimizer and do inference on that model and show the results.\n",
    "\n",
    "#### 1. ONNX model in Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otfT6EDk03KV"
   },
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "\n",
    "# Load network to the plugin\n",
    "ie = IECore()\n",
    "net_onnx = ie.read_network(model='fastseg.onnx')\n",
    "exec_net_onnx = ie.load_network(network=net_onnx, device_name=\"CPU\")\n",
    "del net_onnx\n",
    "\n",
    "input_layer_onnx = next(iter(exec_net_onnx.input_info))\n",
    "output_layer_onnx = next(iter(exec_net_onnx.outputs))\n",
    "\n",
    "# Run the Inference on the Input image...\n",
    "res_onnx = exec_net_onnx.infer(inputs={input_layer_onnx: input_image})\n",
    "res_onnx = res_onnx[output_layer_onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "q8NRH8hLuWeV",
    "outputId": "8b17f90e-b3bc-456f-b6f3-e9cd4f743b2f"
   },
   "outputs": [],
   "source": [
    "result_mask_onnx = np.squeeze(np.argmax(res_onnx, axis=1))\n",
    "show_image_and_result(image, result_mask_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnEiEbNq4Csh"
   },
   "source": [
    "#### 2. IR model in Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otfT6EDk03KV"
   },
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "\n",
    "# Load network to the plugin\n",
    "ie = IECore()\n",
    "net_ir = ie.read_network(model=ir_fname)\n",
    "exec_net_ir = ie.load_network(network=net_ir, device_name=\"CPU\")\n",
    "del net_ir\n",
    "\n",
    "input_layer_ir = next(iter(exec_net_ir.input_info))\n",
    "output_layer_ir = next(iter(exec_net_ir.outputs))\n",
    "\n",
    "# Run the Inference on the Input image...\n",
    "res_ir = exec_net_ir.infer(inputs={input_layer_ir: input_image})\n",
    "res_ir = res_ir[output_layer_ir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "q8NRH8hLuWeV",
    "outputId": "8b17f90e-b3bc-456f-b6f3-e9cd4f743b2f"
   },
   "outputs": [],
   "source": [
    "result_mask_ir = np.squeeze(np.argmax(res_ir, axis=1))\n",
    "show_image_and_result(image, result_mask_ir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3UUduQEGsQm"
   },
   "source": [
    "## PyTorch Comparison\n",
    "\n",
    "Do inference on the PyTorch model to verify that the output visually looks the same as the ONNX/IR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "1l1JtgeV4Wuw",
    "outputId": "f21c8904-83da-438c-df39-4620bb679554"
   },
   "outputs": [],
   "source": [
    "result_torch = model(input_image_tensor)\n",
    "result_mask_torch = torch.argmax(result_torch, dim=1).squeeze(0)\n",
    "show_image_and_result(image, result_mask_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison\n",
    "\n",
    "Measure the time it takes to do inference on five images. This gives an indication of performance. For more accurate benchmarking, use the [OpenVINO benchmark tool](https://github.com/openvinotoolkit/openvino/tree/master/inference-engine/tools/benchmark_tool). Note that many optimizations are possible to improve the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "for _ in range(5):\n",
    "    exec_net_onnx.infer(inputs={input_layer_onnx: input_image})\n",
    "end = time.perf_counter()\n",
    "time_onnx = end - start\n",
    "print(f'ONNX model in Inference Engine: {time_onnx:.3f} seconds')\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(5):\n",
    "    exec_net_ir.infer(inputs={input_layer_ir: input_image})\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "print(f'IR model in Inference Engine: {time_ir:.3f} seconds')\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(5):\n",
    "    model(input_image_tensor)\n",
    "end = time.perf_counter()\n",
    "time_torch = end - start\n",
    "print(f'PyTorch model: {time_torch:.3f} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_ONNX_Support.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OpenVINO 2021.3 PIP installer - PyTorch Image Segmentation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "clean_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
